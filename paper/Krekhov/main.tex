\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[english, russian]{babel}   % use 8-bit T1 fonts
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{algorithm}
\usepackage{algorithmic}

\newtheorem{theorem}{Теорема}
\newtheorem*{theorem-non}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem*{lemma-non}{Лемма}
\newtheorem{assumption}{A}
\newtheorem*{assumption-non}{A}
\newtheorem{corollary}{Следствие}
\newtheorem*{corollary-non}{Следствие}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{subfig}
\DeclareMathOperator\supp{supp}


\usepackage{hyphenat}

\usepackage{comment} %comments
% Your figures are here:
\graphicspath{ {fig/} {../fig/} }


\title{Multiple learning in recommender systems}

\author{
	Nickolai Krekhov \\
	MIPT \\
        Dolgoprudny, Russia \\
        \texttt{krekhov.na@phystech.edu} \\
	%% examples of more authors
	\And
	Andrey Veprikov \\
	Department of Intelligent Systems
            \\MIPT\\Dolgoprudny, Russia\\\texttt{veprikov.as@phystech.edu}\\
	\And
	Anton Khritankov \\
        HSE University, MIPT\\
	Moscow, Russia\\
        \texttt{akhritankov@hse.ru}
}
\date{}


\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
        This paper addresses the issue of evaluating the quality of recommender systems in the long term, taking into account the evolution of consumers and product assortments. We consider the dynamical system of changes in consumers and products over time. The main purpose of the study is to identify the conditions under which degeneracies in audience, assortment, or transaction distribution occur in a given repeated machine learning system, and how such phenomena depend on the learning algorithms and recommendation models. Using the obtained results, we can present a model that is able to increase the metrics in the recommendation systems without degenerating the distributions on products and customers. We conduct a series of computational experiments on the synthetic datasets, the results of the experiments correspond to the theoretical predictions derived from the dynamical model
\end{abstract}


\keywords{Repeated Machine Learning \and Feedback loop}

\section{Introduction}

Recommender systems are an essential component of many online services, serving as powerful tools to offer users the most relevant and personalized content. These systems leverage various machine learning techniques to analyze user preferences, behaviors, and interactions, with the goal of providing tailored recommendations that enhance user experience and engagement. The widespread adoption of recommender systems across diverse domains, such as e-commerce, news platforms, and social networks, underscores their pivotal role in shaping modern digital experiences.

The importance of recommender systems is well-documented in numerous studies~\cite{UserModeling, EuropeanParliament}. These systems have the ability to adapt to user preferences and interests, making them highly valuable for organizations seeking to improve customer satisfaction and drive revenue growth. However, the dynamic nature of recommender systems also introduces potential challenges, such as the emergence of "filter bubbles" and feedback loops that can lead to suboptimal results~\cite{DominicSpohr, krauth2022breaking}

One particularly intriguing aspect of recommender systems is the phenomenon of multiple training, wherein data becomes available sequentially and is used to iteratively improve the system's predictive capabilities. This iterative learning process can have unexpected effects, potentially resulting in the degeneration of the distribution of original items~\cite{KhritankovFeedbackLoops}. Addressing this issue is a crucial concern, as maintaining a diverse and representative item distribution is essential for providing users with a balanced and enriching recommendation experience.

Existing literature has explored various methods for optimizing recommendation metrics while ensuring the stability of distribution patterns over time. Researchers have approached the problem from different angles, such as addressing input data bias to improve algorithm performance~\cite{krueger2020hidden} and identifying sufficient conditions for when dynamic systems with multiple training can lead to distribution degeneration~\cite{inproceedings}. However, a comprehensive mathematical model for the process of multiply learning in recommender systems has not been previously discussed in the literature.

In this article, we propose a novel mathematical model that aims to capture the dynamics of multiple training in recommender systems. Our primary goal is to develop an algorithm that can enhance known recommendation metrics, such as nDCG, RMM, and map@K~\cite{TowardsConversational, wang2013theoretical}, while simultaneously preventing the degeneration of user and item distributions. By addressing this critical challenge, we seek to contribute to the ongoing efforts to improve the reliability, fairness, and sustainability of recommender systems in the digital landscape.

\section{Постановка задачи}

Цель работы --- предложить алгоритм, который улучшает стандартные метрики для рекомендательных алгоримтов (p@K, map@K, nDCG, MRR) при условии, что не возникает вырождения распределения товаров и пользователей, или доказать, что такого не существует. 

Покупатели $c$ и товары $w$ описываются конечным числом признаков, то есть $ \mathbf{c} \in C \subset \mathbb{R}^{d_c}, \mathbf{w} \in W \subset \mathbb{R}^{d_w}$.\\
На каждом шаге $t$ имеется совместное распределение: $(\mathbf{c}, \mathbf{w})^T \sim p^{t}_{c,w} (\mathbf{x_c} , \mathbf{x_w} )$ . Изменение этого распределения от шага $t$ к шагу $t + 1$ определяет оператор эволюции $\text{D}_t : \textbf{R}_{d_c + d_w} \rightarrow \textbf{R}_{d_c + d_w} $, где $R_n$ --- множество всех функций плотности на $\mathbb{R}^n:$
\begin{center}
$
        \mathbf{R_n} := \left\{f : \mathbb{R}^n \rightarrow \mathbb{R}_+ ~\text{and}~ \int\limits_{\mathbb{R}^n}f(x)dx = 1\right\}
        $
\end{center}

Таким образом, $p^{t + 1}(\mathbf{x_c}, \mathbf{x_w}) = \text{D}_t(p^{t})(\mathbf{x_c}, \mathbf{x_w})$.\\
\hspace*{10px}Введем функцию $u_{\text{true}}: \mathbb{R}^{d_c} \times \mathbb{R}^{d_w} \times \Omega_z \to [0; 1]$, которая для потребителя с признаками $\mathbf{x_c} \in \mathbb{R}^{d_c}$, товара с признаками $\mathbf{x_w} \in \mathbb{R}^{d_w}$ и каких-то неизвестных для нас признаков $x_z \in \Omega_z$ определяет вероятность совершения сделки . В дальнейшем будем называть ее истинной функцией полезности\\
Рассматриваемый класс рекомендательных алгоримтов:
\begin{enumerate}
    \item  Cтроит функцию $u_{\text{pred}}: \mathbb{R}^{d_c} \times \mathbb{R}^{d_w} \to [0; 1]$, которая оценивает функцию полезности $u_{\text{true}}(c, w, z)$
    \item Для каждого $c$ по функци $u_{\text{pred}}$ выбирает множество товаров размера k для рекомендации: $\{w_{i_1},\ldots, w_{i_k}\}$
\end{enumerate}

Рекомендательному алгоритму неизвестны $x_z$, $\Omega_z$ и функция $u_{\text{true}}$. 
Рассмотрим множество сделок $F =  \left\{(c, w, u_{\text{true}}(c, w))^T | c, w \sim p_{c, w}(\mathbf{x_c}, \mathbf{x_w})\right\} \subseteq \mathbb{R}^{d_c + d_w + 1}$ и введем на нем функцию $p_u((\mathbf{x_c}, \mathbf{x_w}, u)^T) := u - u_{\text{pred}}(\mathbf{x_c}, \mathbf{x_w}) \in \mathbb{R} \\$
\hspace*{10px}Будем говорить, что распределение $p(x)$ вырождается, если 

% $$ \mu(\supp p())$$
$$
\exists \phi(x) : \supp p(x) = \{x \; |\; \phi(x) = 0\}
$$

Это означает, что $\phi(x)$ является уравнением поверхности меньшей размерности, которая и является носителем распределения $p(x)$. Стоит отметить, что раз вся вероятностная мера сосредоточна на подпространств меньшей размерности, то значение функции распределения на этом подпространстве будет равно бесконечности.

Множеством вырождения будем называть $\Phi = \{x \; |\; \phi(x) = \infty\}$

Введем функцонал качества:
            $$L^t(c, w) = \mathbb{E}_z[(u_{\text{true}}(c, w, z) - u_{\text{pred}}(c, w))^2],$$




Улучшение метрик означает, что должно происходить вырождение распределения на множестве $F$, однако тут в качестве $\phi$ мы возьмем конкретную функцию, а именно $p_u(x)$.

Итак, нужно предложить алгоритм, при использовании которого:
\begin{enumerate}
    \item $\neg \exists \phi(x) : \supp p_{c, w}^{\infty}(\mathbf{x_c}, \mathbf{x_w}) = \{x \; |\; \phi(x) = 0\}$
    \item $p_u^{\infty}((\mathbf{x_c}, \mathbf{x_w}, u)^T) = \delta(x)$, где $\delta(x)$ - дельта-функция Дирака
\end{enumerate}
или доказать, что такого не существует.

\subsection{Критерии качества модели}
Важным признаком для сравнения качества моделей является невырождение распределения $p^t_{c,w}$ пользователей-товаров. Мы должны найти такой алгоритм, при использовании
которого в динамической системе не будет происходит вырождения.
\begin{enumerate}
    \item Вырождение распределения невязок: $u_{\text{true}} - u_{\text{pred}} \sim \delta(x)$, где $\delta(x)$ - дельта-функция Дирака.\\
    Условия такого вырождения описаны в статье \cite{veprikov2024mathematical}, однако никаких гарантий на отсутствие выраждения $p^t_{c,w}$ нет.
    
    \item $y_{\text{true}} := Bern(u_{\text{true}})$, 
    $y_{\text{pred}} := Bern(u_{\text{pred}})$\\
    Для каждого пользователя считаем $accuracy@K = \frac{\sum^K_{k = 1} (\mathbf{I}\{y^k_{\text{pred}} = y^k_{\text{true}}\})} {K}$ и затем усредняем по всем пользователям.

\end{enumerate}

\section{Основные результаты}
Для доказательства утверждений нужно сделать важное предположения о поведении пользователей и площадок с товарами.\\



{\bf Предположение 1}:
Функция интереса $u_{\text{true}}: \mathbb{R}^{d_c} \times \mathbb{R}^{d_w} \times \Omega_z \to \mathbb{R}$ существует. 

{\bf Предположение 2}:
Пользователи и площадка с товарами ведут себя рационально, т.е. $p^{t+1}_{c, w}  \propto L^t(c, w)^{-1}$.
\\
\begin{theorem}

    \item Пусть выполнены Предположение 1 и Предположение 2. Тогда в зависимости от $u_{\text{pred}}$ множество $\Phi^t(x)$ будет иметь следующий вид:

    \begin{enumerate}

    \item $u^1_{\text{pred}}(c, w) = \mathbb{E}_z[u_{\text{true}}(c, w, z)]$, \\
    % тогда $L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)]$\\
    $\Phi^t_1 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \mathbb{D}_z[(u_{\text{true}}(c, w, z)]=0 \; \right\}$\\


    \item
    $u^2_{\text{pred}}(c, w)$ = 
    \begin{cases}
       1, &\text{$\mathbb{E}_z[u_{\text{true}}(c, w, z)] \geq \frac{1}{2} $}\\
       0, &\text{иначе}
    \end{cases}, \\
    % $L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \min \left(\mathbb{E}_z[(u_{\text{true}}(c, w, z)]^2; 1 -  \mathbb{E}_z[(u_{\text{true}}(c, w, z)]^2 \right)$\\
 
        $\Phi^t_2 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \text{для п.в. $x_z \in \Omega_z$ $u_{\text{true}}(c, w, z) = 1$ или 0 } \; \right\}$ \\

    \item  $u^3_{\text{pred}}(c, w) = a = const$, \\
    $\Phi^t_3 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \text{для п.в. $x_z \in \Omega_z$ $u_{\text{true}}(c, w, z) = a$} \; \right\}$ \\
    \end{enumerate}
    
\end{theorem}

Заметим, что выполнены вложения $\Phi^t_2 \subset \Phi^t_1$ и $\Phi^t_3 \subset \Phi^t_1$. Обратим внимание, что на точки, подходящие для $\Phi^t_2$ или $\Phi^t_3$ накладываются достаточно сильные ограничения. Это позволяет выдвинуть гипотезу, что вырождения при $u_{\text{pred}}(c, w) := u^2_{\text{pred}}(c, w)$ не будет при соответствующих ограничениях на $u_{\text{true}}$. В то время как в первом 

\begin{lemma}
    \item Пусть $\xi, \eta$ - случайные величины, $\mathbb{P} \left\{0 \leq \xi,\eta \leq 1 \right\} = 1$, 
    $\hat{\xi} \sim Bern(\xi)$, $\hat{\eta} \sim Bern(\eta)$ \\
    Тогда $\argmax_{\mathbb{E}[\eta]}\mathbb{P}\{|\hat{\xi} - \hat{\eta}| = 0\}$ = 
    \begin{cases}
       1, &\text{$\mathbb{E}[\xi] \geq \frac{1}{2} $}\\
       0, &\text{иначе}
    \end{cases}
\end{lemma}

Сделаем следующую подстановку: $$\xi = u_{\text{true}}(c, w, z),$$
$$\eta = u_{\text{pred}}(c, w)$$
Согласно Лемме 1: $u_{\text{pred}}(c, w) := u^2_{\text{pred}}(c, w)$ из Теоремы 1 должен будет максимизировать метрику accuracy@K.

\section{Вычислительный эксперимент} 
Целью эксперимента является проверка теоретических утверждений, полученных в этой работе. Мы проверим гипотезы о вырождении и не вырождении распределений в зависимости от функции $u_{\text{pred}}$ из Теоремы 1. Для этого мы будем считать дисперсию выборок товаров и пользователей на каждом шаге, значение функционала L, а также строить графики распределений пользователей и товаров. Проверим выполнение леммы о том, что $u_{\text{pred}}(c, w)$ = 
    \begin{cases}
       1, &\text{$\mathbb{E}_z[(u_{\text{true}}(c, w, z)] \geq \frac{1}{2} $}\\
       0, &\text{иначе}
    \end{cases} максимизирует метрику accuracy@K. Также будем считать метрику precision@K.
\subsection{Данные}
Данные используются синтетические, пользователи и товары имеют по одному параметру:
        $\\
        c \sim \mathcal{N}(0.7, 0.3), \qquad
        w \sim \mathcal{N}(0, 0.6),  \qquad
        z \sim \mathcal{N}(0, 0.05),  \qquad
        $\\
    Эксперимент будет проводиться для двух функций $u_{\text{true}}(c,w,z)$.



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{images/21_05_exp_uni/loss.png}
    \caption{Функция потерь для базового алгоритма рекомендаций}
    \label{loss}
\end{figure}

\subsection{Описание эксперимента}
В качестве базового алгоритма рекомендаций, оценивающего $u_{\text{true}}$, будем использовать нейронную сеть.
Будем запускать динамическую во времени систему на 100 итераций. Одна итерация выглядит следующим образом:
\begin{enumerate}
\item Происходит сэмплирование выборок произвольного размера покупателей и товаров из распределений $f_c^t$ и $f_w^t$:
$c_1^t,..,c_n^t \sim f_c^t,\ w_1^t,..,w_k^t \sim f_w^t$.
\item Рекомендательная система подбирает для каждого покупателя из выборки подмножество товаров из $w_1^t,..,w_k^t$
\item Покупатели совершают покупку с вероятностью $u_{\text{true}}(c, w, z)$ Далее рекоммендательная система дообучается на полученном фидбэке, используя базовый алгоритм рекомендательный системы.
\item Шаги 2-3 повторяются произвольное количество раз.
\item Далее проиходит обновление распределений товаров и пользователей в соответствии со значениями функции $L$ в точках  $(c, w)$, $c \in \{ c_1^t,..,c_n^t \}, \; w \in \{ w_1^t,..,w_k^t \}$ . Будем это делать при помощи ядерной оценки плотности.
\end{enumerate}

Рассмотрим следующие функции полезности:
\begin{enumerate}
% \item $u_{\text{true}}(c, w, z) = \frac{\arctan(c - w + z)}{\pi} + 0.5$ \\
% Ожидаем, что распределния будут сдвигаться, так как чем больше значение $c-w$, тем более вероятна покупка, поэтому ожидается, что математическое ожидание признака пользователя будет увеличиваться, а признака предетов уменьшаться.
\item$u_{\text{true}}(c, w, z) = \exp{(-0.5((c - c_0)^2 + (c - w_0)^2 + z^2))}, c_0 = 0.1, w_0 = 0.9 $\\
Мы рассматриваем такую эту функцию, так как она унимодальна и достигает своего максимума в точке $(c_0, w_0)$.
Ожидаем, что если распределния и будут вырождаться, то в точке $(c_0, w_0)$, так как в ней достигается наибольшее значение функции полезности. 
\end{enumerate}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\linewidth]{images/21_05_exp_uni/distr_case_1.png}
    \includegraphics[width=0.49\linewidth]{images/21_05_exp_uni/distr_case_2.png}
    \includegraphics[width=0.49\linewidth]{images/21_05_exp_uni/distr_random.png}
    \caption{Распределения после 160 итераций.  $u^1_{\text{pred}}$ (left), $u^2_{\text{pred}}$ (right), $u^3_{\text{pred}}=random$}
    \label{distributions_1}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/customer_variance.png}
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/item_variance.png}
    
    \caption{Дисперсия параметра пользователя (слева), товара (справа) в зависимости от номера итерации}
    \label{variances}
\end{figure}

\subsection{Результаты эксперимента}
\begin{enumerate}
    \item По функции потерь на Рис. \ref{loss} видно, что обе модели обучились, но при $u^2_{\text{pred}}(c, w)$ = 
    \begin{cases}
       1, &\text{$\mathbb{E}_z[u_{\text{true}}(c, w, z)] \geq \frac{1}{2} $}\\
       0, &\text{иначе}
    \end{cases} базовая рекомендательная модель обучилась лучше.
    
    На Рис. \ref{distributions_1} показаны распределения на конкретных итерациях. Как и ожидалось, для $u^1_{\text{pred}} = \mathbb{E}_z[u_{\text{true}}(c, w, z)] $ происходит вырождение распределение товаров, а для функции  $u^2_{\text{pred}}(c, w)$ вырождения нет. Это видно по графикам дисперсии пользователей, товаров (Рис. \ref{variances})и графику функционала $L(c, w)$ (Рис. \ref{L_f} слева) . Дисперсия и значение функционала $L$ при $u^1_{\text{pred}}(c, w)$ меньше, чем у $u^2_{\text{pred}}(c, w)$. Это означает, что вырождение происходит намного быстрее при $u^1_{\text{pred}}(c, w)$. 
    
    Теперь посмотрим на значения метрики accuracy@8.
    Видим, с некоторого номера значение метрики accuracy@8 у $u^2_{\text{pred}}(c, w)$ больше чем у  $u^1_{\text{pred}}(c, w)$, что соответсвует полученным нами теоретическим результатам в Лемме 1.

    На Рис. \ref{L_f} (справа) показано значение распределения невязок в точке 0. Как мы видим, происходит вырождения невязок при $u^1_{\text{pred}}(c, w)$, это говорит о том, что $u^1_{\text{pred}}(c, w)$ лучше предсказывает функцию полезности.
\end{enumerate}




\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/L.png}
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/f.png}
    \caption{Значение функционала качества $L(c, w)$ (слева) и функции распределения невязок в точке 0 (справа)}
    \label{L_f}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/accuracy.png}
    \includegraphics[width=0.35\linewidth]{images/21_05_exp_uni/precision.png}
    
    \caption{accuracy@8 (слева), precision@8 (справа)}
    \label{accuracy}
\end{figure}

% \clearpage
\section{Заключение}

В данной статье была построена математическая модель динамической во времени рекомендательной системы. Для определенного вида алгоримтов нам удалось найти множества вырождения пользователей-товаров. Был предложен алгоритм, при использовании которого увеличивается устойчивость к негативным эффектам, возникаемым в динамических системах. Проведен вычислительный эксперимент на синтетических данных, который подтвердил теоретические результаты. \\
В дальнейшем планируется рассмотреть другие функционалы качества и провести эксперименты на реальных данных.

\newpage
\bibliographystyle{plain}
\bibliography{refs}  

\begin{center}
    \LARGE \textbf{Appendix}
\end{center}
\normalsize

  

\appendix

        \section{Proof of Theorem 1} 
    \begin{proof}

        $$L^t(c, w) = \mathbb{E}_z[(u_{\text{true}}(c, w, z) - u_{\text{pred}}(c, w))^2]$$ переписывается в виде:
        $$L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \left(\mathbb{E}_z[(u_{\text{true}}(c, w, z)] - u_{\text{pred}}(c, w)\right)^2,$$ 
            $$L^t(\mathbf{x_c}, \mathbf{x_w}) = \mathbb{E}_z \left[(u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z)-u_{\text{pred}})^2 \right] =  $$ 
            $$ = \mathbb{E}_z \left[(u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) -  \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right] + \mathbb{E}_z \left[ u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right] - u_{\text{pred}}(\mathbf{x_c}, \mathbf{x_w}))^2 \right] = $$ 
            $$ = \mathbb{E}_z \left[(u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) -  \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right])^2\right] + \mathbb{E}_z \left[( \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right] -  u_{\text{pred}}(\mathbf{x_c}, \mathbf{x_w}))^2\right]  + $$ 
            $$ + 2\mathbb{E}_z \left[u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z)
            - \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right]\right] \cdot  (\mathbb{E}_z \left[ u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right] - u_{\text{pred}}(\mathbf{x_c}, \mathbf{x_w})) = $$ $$ = \mathbb{D}_z \left[ u_{\text{true}}\right] + ( \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right] -  u_{\text{pred}}(\mathbf{x_c}, \mathbf{x_w}))^2 , $$
        
        так как 
        $$\mathbb{E}_z \left[u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) -  \mathbb{E}_z \left[  u_{\text{true}}(\mathbf{x_c}, \mathbf{x_w}, x_z) \right]\right] = 0. $$

        \begin{enumerate}

            \item $u^1_{\text{pred}}(c, w) = \mathbb{E}_z[u_{\text{true}}(c, w, z)]$, \\
            $$L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \left(\mathbb{E}_z[u_{\text{true}}(c, w, z)] -  \mathbb{E}_z[u_{\text{true}}(c, w, z)]\right)^2 = $$
            $$ = \mathbb{D}_z[(u_{\text{true}}(c, w, z)]$$\\
            Таким образом, получаем, что
            $$\Phi^t_1 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \mathbb{D}_z[(u_{\text{true}}(c, w, z)]=0 \; \right\}$$\\
                
            \item
            $u^2_{\text{pred}}(c, w)$ = 
            \begin{cases}
               1, &\text{$\mathbb{E}_z[u_{\text{true}}(c, w, z)] \geq \frac{1}{2} $}\\
               0, &\text{иначе}
            \end{cases}, \\
            $$L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \left(\mathbb{E}_z[u_{\text{true}}(c, w, z)] - \max(\mathbb{E}_z[u_{\text{true}}(c, w, z)], 1 - \mathbb{E}_z[u_{\text{true}}(c, w, z)]) \right)^2 = $$
            $$ = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \min \left(\mathbb{E}_z[(u_{\text{true}}(c, w, z)]^2; 1 -  \mathbb{E}_z[(u_{\text{true}}(c, w, z)]^2 \right)$$\\
         
                $\Phi^t_2 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \text{для п.в. $x_z \in \Omega_z$ $u_{\text{true}}(c, w, z) = 1$ или 0 } \; \right\}$ \\
            
            \item  $u^3_{\text{pred}}(c, w) = a = const$, \\
            $$L^t(c, w) = \mathbb{D}_z[(u_{\text{true}}(c, w, z)] + \left(\mathbb{E}_z[u_{\text{true}}(c, w, z)] -  a\right)^2 = $$
            
            $\Phi^t_3 = \left\{ (\mathbf{x_c}, \mathbf{x_w})^T \in \mathbb{R}^{d_c + d_w} \; | \; \text{для п.в. $x_z \in \Omega_z$ $u_{\text{true}}(c, w, z) = a$} \; \right\}$ \\

            
        \end{enumerate}
    \end{proof}



    \section{Proof of Lemma 1} 
    Пусть $\xi, \eta$ - случайные величины, $\mathbb{P} \left\{0 \leq \xi,\eta \leq 1 \right\} = 1$, 
    $\hat{\xi} \sim Bern(\xi)$, $\hat{\eta} \sim Bern(\eta)$ \\
    Тогда $\argmax_{\mathbb{E}[\eta]}\mathbb{P}\{|\hat{\xi} - \hat{\eta}| = 0\}$ = 
    \begin{cases}
       1, &\text{$\mathbb{E}[\xi] \geq \frac{1}{2} $}\\
       0, &\text{иначе}
    \end{cases}
    $
    \begin{proof}

    Заметим, что 
    $$\mathbb{E}[\xi] = 1 \cdot \mathbb{P}\{\xi = 1\} + 0 \cdot \mathbb{P}\{\xi = 0\} = \mathbb{P}\{\xi = 1\}, 
    \mathbb{P}\{\xi = 0\} = 1 - \mathbb{E}[\xi]$$
    $$\mathbb{P}\{|\hat{\xi} - \hat{\eta}| = 0\} = \mathbb{P}\{\hat{\xi} = \hat{\eta}\} = \mathbb{P}\{\xi = 1\} \mathbb{P}\{\eta = 1\} + \mathbb{P}\{\xi = 0\}\mathbb{P}\{\eta = 0\} = \mathbb{E}[\xi] \mathbb{E}[\eta] + (1 - \mathbb{E}[\xi])(1 - \mathbb{E}[\eta])$$
    Это линейная функция от $\mathbb{E}[\eta]$, следовательно ее максимум достигается на на одном из концов, т.е.
    $$\max_{\mathbb{E}[\eta]}(\mathbb{E}[\xi] \mathbb{E}[\eta] + (1 - \mathbb{E}[\xi])(1 - \mathbb{E}[\eta])) = \max\{\mathbb{E}[\xi], 1 - \mathbb{E}[\xi]\}$$
    
    

    \end{proof}

    
\end{document}


